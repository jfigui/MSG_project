{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing for Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we prepare the data that is going to be ingested by the random forest model. A total of 6 features are going to be used: HRV_norm, IR_108, channel differences (WV_062-IR_108) and their textures. The IR_108 contains information about the cloud top height, the channel differences contain information about the water content of the cloud and the HRV provides information about the structure of the cloud. We use the texture of these variables to introduce some information about the spatial structure in this pixel-based retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pyart\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/exts-ml2/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# suppress anoying iypthon warnings. Not ideal since we suppress also potentially relevant warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fbasepath_input = '/data/pyrad_products/MSG_ML/'\n",
    "fbasepath_output = '/data/ml_course/05_Capstone_project/rf_data/'\n",
    "variables = ['HRV', 'HRV_norm', 'HRV_text', 'HRV_norm_text', 'IR_108', 'IR_108_text', 'WV_062-IR_108', 'WV_062-IR_108_text', 'POH90']\n",
    "\n",
    "years = ['2018', '2019', '2020']\n",
    "months = ['04', '05', '06', '07', '08', '09']\n",
    "\n",
    "features = ['HRV_norm', 'HRV_norm_text', 'IR_108', 'IR_108_text', 'WV_062-IR_108', 'WV_062-IR_108_text']\n",
    "target = 'POH90'\n",
    "nfeatures = len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read original dataset\n",
    "# data is stored as (nz, ny, nx), we return (nx, ny)\n",
    "def read_nc(fname):\n",
    "    sat_grid = pyart.io.read_grid(fname)\n",
    "    for field_name in sat_grid.fields.keys():\n",
    "        data = np.transpose(np.squeeze(sat_grid.fields[field_name]['data']))\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data where IR temperature too high or channel differences too negative\n",
    "def filter_data(X, y, ir_thr=240, diff_thr=-50):\n",
    "    # Find row indexes of elements to delete\n",
    "    ind = []\n",
    "    ind_ir = np.where(X[:, 2] > ir_thr)\n",
    "    if len(ind_ir[0]) > 0:\n",
    "        ind.extend(ind_ir[0])\n",
    "    ind_diff = np.where(X[:, 4] < diff_thr)\n",
    "    if len(ind_diff[0]) > 0:\n",
    "        ind.extend(ind_diff[0])\n",
    "    if len(ind) == 0:\n",
    "        return X, y\n",
    "    \n",
    "    # delete from feature matrix\n",
    "    X2 = np.delete(X, ind, axis=0)\n",
    "    \n",
    "    # delete from target matrix\n",
    "    y2 = np.delete(y, ind, axis=0)\n",
    "    \n",
    "    return X2, y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HRV 4353\n",
      "HRV_norm 4353\n",
      "HRV_text 4353\n",
      "HRV_norm_text 4353\n",
      "IR_108 4355\n",
      "IR_108_text 4355\n",
      "WV_062-IR_108 4355\n",
      "WV_062-IR_108_text 4355\n",
      "POH90 4355\n"
     ]
    }
   ],
   "source": [
    "# Check number of files for each variable\n",
    "for var in variables:\n",
    "    flist = glob.glob(fbasepath_input+'*/NETCDF/'+var+'/*.nc')\n",
    "    print(var, len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will identify the missing HRV files\n",
    "#flist = glob.glob(fbasepath+'*/NETCDF/'+variables[-1]+'/*nc')\n",
    "#flist.sort()\n",
    "#for fname in flist:\n",
    "#    bfile = os.path.basename(fname)\n",
    "#    dt_str = bfile[0:14]\n",
    "#    print(dt_str, end=\"\\r\", flush=True)\n",
    "#    for var in vars:\n",
    "#        flist_aux = glob.glob(fbasepath+'*/NETCDF/'+var+'/'+dt_str+'*.nc')\n",
    "#        if len(flist_aux) > 0:\n",
    "#            pass\n",
    "#            # print(flist_aux[0])\n",
    "#        else:\n",
    "#            print(dt_str, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep only data where potentially hail could be located. That means clouds that have a high top (low IR_108 values) and small channel differences (high water content). The features are stored in a feature matrix with 6 columns. A file per month is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200731173000\r"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    for month in months:\n",
    "        # Get list of files and data size\n",
    "        flist = glob.glob(fbasepath_input+'*/NETCDF/'+features[0]+'/'+year+month+'*.nc')\n",
    "        if len(flist) == 0:\n",
    "            continue\n",
    "        flist.sort()\n",
    "        img_size = read_nc(flist[0]).shape\n",
    "        data_size = img_size[0]*img_size[1]\n",
    "        \n",
    "        X = None\n",
    "        for fname in flist:\n",
    "            # Get time step\n",
    "            bfile = os.path.basename(fname)\n",
    "            dt_str = bfile[0:14]\n",
    "            print(dt_str, end=\"\\r\", flush=True)\n",
    "            \n",
    "            # Read all files corresponding to a time step\n",
    "            # Put them in features and target matrices\n",
    "            X_aux = np.empty((data_size, nfeatures), dtype=np.float32)\n",
    "            for i, feature in enumerate(features):\n",
    "                flist_aux = glob.glob(fbasepath_input+'*/NETCDF/'+feature+'/'+dt_str+'*.nc')\n",
    "                data = read_nc(flist_aux[0]).flatten()\n",
    "                X_aux[:, i] = data\n",
    "               \n",
    "            flist_aux = glob.glob(fbasepath_input+'*/NETCDF/'+target+'/'+dt_str+'*.nc')\n",
    "            y_aux = np.transpose(read_nc(flist_aux[0]).flatten())\n",
    "            \n",
    "            # Filter data\n",
    "            X_aux, y_aux = filter_data(X_aux, y_aux)\n",
    "            \n",
    "            # Put all data together\n",
    "            if X is None:\n",
    "                X = X_aux\n",
    "                y = y_aux\n",
    "            else:\n",
    "                X = np.concatenate((X, X_aux), axis=0)\n",
    "                y = np.concatenate((y, y_aux), axis=0)\n",
    "                \n",
    "        # Save data into a .npz file\n",
    "        np.savez(fbasepath_output+year+month+'_data.npz', features=X, targets=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
