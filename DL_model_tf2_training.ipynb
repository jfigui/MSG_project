{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-NET model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses the unet package cloned from https://github.com/jakeret/unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.3.0\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import unet\n",
    "from unet import utils\n",
    "from unet.datasets import circles\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import inspect\n",
    "\n",
    "print(\"TF version:\", tf.__version__)\n",
    "#print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = '/data/ml_course/05_Capstone_project/dl_data/'\n",
    "modelpath = './unet_ld3_fr16'\n",
    "callbackpath = './unet_ld3_fr16_callbacks'\n",
    "flist_name = 'unet_ld3_fr16_flist.npz'\n",
    "hist_name = 'unet_ld3_fr16_history.pickle'\n",
    "\n",
    "# Number of files used for training, validation and testing\n",
    "nfiles_te = 4\n",
    "nfiles_va = 4\n",
    "nfiles_tr = 36\n",
    "\n",
    "# Model architecture\n",
    "layer_depth = 3\n",
    "filters_root = 16\n",
    "\n",
    "# parameters for training and evaluation\n",
    "train_batch_size = 4\n",
    "pred_batch_size = 4\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    \"\"\"\n",
    "    Reads features and labels stored in npz files\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str\n",
    "        path of the file containing the data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : float array \n",
    "        features matrix of size nx, ny, nchannels\n",
    "    y : float array\n",
    "        lables matrix of size, nx, ny, nclasses\n",
    "    \n",
    "    \"\"\"\n",
    "    with np.load(fname, allow_pickle=False) as npz_file:\n",
    "        # Load the arrays\n",
    "        X = npz_file['features']\n",
    "        y = npz_file['targets']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(file_list, stop_at_end=False):\n",
    "    \"\"\"\n",
    "    data generator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list : list of str\n",
    "        lists of files where the data is stored\n",
    "    stop_at_end : bool\n",
    "        Controls the behaviour when running out of files.\n",
    "        If True exits the function. Otherwise reshuffles the list\n",
    "        and sets the counter to 0\n",
    "    \n",
    "    Yield\n",
    "    -------\n",
    "    X : float array \n",
    "        features matrix of size nx, ny, nchannels\n",
    "    y : float array\n",
    "        lables matrix of size, nx, ny, nclasses\n",
    "    \n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        if i >= len(file_list):\n",
    "            if stop_at_end:\n",
    "                break\n",
    "            i = 0\n",
    "            np.random.shuffle(file_list)            \n",
    "        else:\n",
    "            X, y = read_data(file_list[i])            \n",
    "            yield X, y\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(flist, stop_at_end=False):\n",
    "    \"\"\"\n",
    "    Creates a tensorflow dataset from a generator\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_list : list of str\n",
    "        lists of files where the data is stored\n",
    "    stop_at_end : bool\n",
    "        Controls the behaviour when running out of files.\n",
    "        If True exits the function. Otherwise reshuffles the list\n",
    "        and sets the counter to 0\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataset : tf.data.Dataset\n",
    "        A dataset containing the features and labels\n",
    "    \n",
    "    \"\"\"\n",
    "    X, y = read_data(flist[0])\n",
    "    nx = X.shape[0]\n",
    "    ny = X.shape[1]\n",
    "    nchannels = X.shape[2]\n",
    "    nclasses = y.shape[2]\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        data_generator, args=[flist, stop_at_end], output_types=(tf.float32, tf.float32),\n",
    "        output_shapes = ((nx, ny, nchannels), (nx, ny, nclasses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_data(flist):\n",
    "    \"\"\"\n",
    "    Get the number of pixels corresponding to each class and the total number\n",
    "    of pixels in a dataset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    flist : list of str\n",
    "        lists of files where the data is stored\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    total : int\n",
    "        The total number of pixels in the dataset\n",
    "    nel_class : array of ints\n",
    "        The number of pixels for each class\n",
    "    \n",
    "    \"\"\"\n",
    "    _, y = read_data(flist[0])    \n",
    "    nclasses = y.shape[2]\n",
    "                 \n",
    "    total = 0\n",
    "    nel_class = np.zeros(nclasses, dtype=np.int)\n",
    "    for i, fname in enumerate(flist):\n",
    "        _, y = read_data(fname)\n",
    "        total += int(y.size/nclasses)\n",
    "        \n",
    "        for j in range (nclasses):\n",
    "            ind = np.where(y[:, :, j] == 1)[0]\n",
    "            nel_class[j] += ind.size\n",
    "    return total, nel_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "    \"\"\"\n",
    "    Computes the weighted binary crossentropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zero_weight, one_weight : float\n",
    "        The weights for each class\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    weigthed_bce : tensor flow array\n",
    "        The weighted loss at each pixel\n",
    "    \n",
    "    \"\"\"\n",
    "    def wbce(y_true, y_pred):\n",
    "        # Calculate binary crossentropy\n",
    "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "        \n",
    "        ## Apply weights\n",
    "        index = tf.cast(tf.argmax(y_true, axis=-1), tf.float32) # from one hot to indices\n",
    "        weight_vector = index*one_weight+(1.-index)*zero_weight\n",
    "        weigthed_bce = weight_vector*bce\n",
    "        \n",
    "        return weigthed_bce\n",
    "    return wbce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files: 4351\n"
     ]
    }
   ],
   "source": [
    "flist = glob.glob(datapath+'*_data.npz')\n",
    "flist.sort()\n",
    "np.random.shuffle(flist)\n",
    "print('Number of input files:', len(flist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test files: 4\n",
      "Number of validation files: 4\n",
      "Number of training files: 36\n"
     ]
    }
   ],
   "source": [
    "flist_te = flist[0:nfiles_te]\n",
    "flist_va = flist[nfiles_te:nfiles_te+nfiles_va]\n",
    "flist_tr = flist[nfiles_te+nfiles_va:nfiles_te+nfiles_va+nfiles_tr]\n",
    "\n",
    "print('Number of test files:', len(flist_te))\n",
    "print('Number of validation files:', len(flist_va))\n",
    "print('Number of training files:', len(flist_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file names for each category:\n",
    "np.savez(flist_name, flist_te=flist_te, flist_va=flist_va, flist_tr=flist_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(flist_tr, stop_at_end=False)\n",
    "validation_dataset = get_dataset(flist_va, stop_at_end=False)\n",
    "test_dataset = get_dataset(flist_te, stop_at_end=True) # Put to stop at end because there is no stopping mechanism in the u-net evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pixels in training dataset: 4128768\n",
      "Number of no hail pixels: 4125295\n",
      "Number of hail pixels: 3473\n",
      "% o hail pixels over total: 0.08411710224454365\n"
     ]
    }
   ],
   "source": [
    "total, nel_class = examine_data(flist_tr)\n",
    "print('Total number of pixels in training dataset:', total)\n",
    "print('Number of no hail pixels:', nel_class[0])\n",
    "print('Number of hail pixels:', nel_class[1])\n",
    "print('% o hail pixels over total:', 100*nel_class[1]/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight for no hail: 0.50\n",
      "Weight for hail: 594.41\n"
     ]
    }
   ],
   "source": [
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "weight_nohail = (1 / nel_class[0])*(total)/2.0 \n",
    "weight_hail = (1 / nel_class[1])*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_nohail, 1: weight_hail}\n",
    "\n",
    "print('Weight for no hail: {:.2f}'.format(weight_nohail))\n",
    "print('Weight for hail: {:.2f}'.format(weight_hail))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = unet.build_model(channels=3,\n",
    "                              num_classes=2,\n",
    "                              layer_depth=layer_depth,\n",
    "                              filters_root=filters_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='binary_accuracy', threshold=0.5),\n",
    "#    tf.keras.metrics.FalseNegatives(name='fn', thresholds=0.5),\n",
    "#    tf.keras.metrics.FalsePositives(name='fp', thresholds=0.5),\n",
    "#    tf.keras.metrics.TrueNegatives(name='tn', thresholds=0.5),\n",
    "#    tf.keras.metrics.TruePositives(name='tp', thresholds=0.5)\n",
    "]\n",
    "\n",
    "loss = weighted_binary_crossentropy(weight_nohail, weight_hail)\n",
    "# loss = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "unet.finalize_model(\n",
    "    unet_model, loss=loss,\n",
    "    metrics=metrics,\n",
    "    dice_coefficient=False,\n",
    "    auc=False,\n",
    "    mean_iou=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/9 [==>...........................] - ETA: 0s - loss: 0.4521 - binary_accuracy: 0.7121WARNING:tensorflow:From /opt/anaconda3/envs/exts-ml2/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "9/9 [==============================] - 17s 2s/step - loss: 0.6841 - binary_accuracy: 0.7671 - val_loss: 0.5629 - val_binary_accuracy: 0.8961 - learning_rate: 0.0010\n",
      "Epoch 2/3\n",
      "9/9 [==============================] - 17s 2s/step - loss: 0.6407 - binary_accuracy: 0.8701 - val_loss: 0.4861 - val_binary_accuracy: 0.9280 - learning_rate: 0.0010\n",
      "Epoch 3/3\n",
      "9/9 [==============================] - 17s 2s/step - loss: 0.5427 - binary_accuracy: 0.9392 - val_loss: 0.3968 - val_binary_accuracy: 0.7733 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "fit_kwargs = {\n",
    "    'steps_per_epoch': int(nfiles_tr/train_batch_size),\n",
    "    'validation_steps': int(nfiles_va/train_batch_size)}\n",
    "\n",
    "trainer = unet.Trainer(log_dir_path=callbackpath, checkpoint_callback=False)\n",
    "history = trainer.fit(unet_model,\n",
    "            train_dataset,\n",
    "            validation_dataset,\n",
    "            epochs=epochs,\n",
    "            batch_size=train_batch_size,\n",
    "            **fit_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history\n",
    "with open(hist_name, 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/exts-ml2/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/exts-ml2/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./unet_ld3_fr16/assets\n"
     ]
    }
   ],
   "source": [
    "unet_model.save(modelpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
